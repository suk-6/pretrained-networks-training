{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "from transform import transform_COCO\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "class COCO_Dataset(Dataset):\n",
    "    def __init__(self, split='TRAIN'):\n",
    "        super().__init__()\n",
    "        self.rootDataset = os.path.join('/', 'home', 'dblab', 'coco')\n",
    "        self.trainAnnoPath = os.path.join(self.rootDataset, 'annotations', 'instances_train2017.json')\n",
    "        self.traincategoryPath = os.path.join(self.rootDataset, 'annotations', 'categories.json')\n",
    "        self.trainImagePath = os.path.join(self.rootDataset, 'train2017')\n",
    "        self.coco = COCO(self.trainAnnoPath)\n",
    "\n",
    "        whole_image_ids = self.coco.getImgIds()  # original length of train2017 is 118287\n",
    "\n",
    "        self.image_ids = []\n",
    "\n",
    "        # to remove not annotated image idx\n",
    "        self.no_anno_list = []\n",
    "\n",
    "        for idx in whole_image_ids:\n",
    "            annotations_ids = self.coco.getAnnIds(imgIds=idx, iscrowd=False)\n",
    "        if len(annotations_ids) == 0:\n",
    "            self.no_anno_list.append(idx)\n",
    "        else:\n",
    "            self.image_ids.append(idx)\n",
    "\n",
    "        self.load_classes() # read class information\n",
    "        self.split = split\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        visualize = True\n",
    "\n",
    "        image, (w, h) = self.load_image(idx)\n",
    "\n",
    "        annotation = self.load_annotations(idx)\n",
    "\n",
    "        boxes = torch.FloatTensor(annotation[:, :4])\n",
    "        labels = torch.LongTensor(annotation[:, 4])\n",
    "\n",
    "        if labels.nelement() == 0:  # no labeled img exists.\n",
    "            visualize = True\n",
    "        # data augmentation\n",
    "        image, boxes, labels, segmentations = transform_COCO(image, boxes, labels, self.split)\n",
    "\n",
    "        return image, boxes, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 경로 설정\n",
    "data_dir = os.path.join('/', 'home', 'dblab', 'coco')\n",
    "train_dir = os.path.join(data_dir, 'train2017')\n",
    "val_dir = os.path.join(data_dir, 'val2017')\n",
    "annotations_path = os.path.join(data_dir, 'annotations', 'instances_train2017.json')\n",
    "\n",
    "# 클래스 정보 로드\n",
    "with open(annotations_path, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# 클래스 ID와 이름 매핑\n",
    "categories = {category['id']: category['name'] for category in annotations['categories']}\n",
    "\n",
    "# 이미지 경로 및 해당하는 라벨 가져오기\n",
    "image_paths = []\n",
    "image_labels = []\n",
    "\n",
    "for image_info in tqdm(annotations['images']):\n",
    "    image_id = image_info['id']\n",
    "    image_path = os.path.join(train_dir, image_info['file_name'])\n",
    "    image_paths.append(image_path)\n",
    "\n",
    "    labels = []\n",
    "    for annotation in annotations['annotations']:\n",
    "        if annotation['image_id'] == image_id:\n",
    "            labels.append(annotation['category_id'])\n",
    "    image_labels.append(labels)\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, image_paths, image_labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.image_labels = image_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        labels = self.image_labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "# 데이터 전처리 및 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "dataset = COCODataset(image_paths, image_labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 사전 훈련된 ResNet50 불러오기\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# ResNet의 마지막 레이어 변경 (수정이 필요할 수 있음)\n",
    "num_classes = len(categories)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# 모델 훈련\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 옵티마이저와 손실 함수 정의 (적절한 것으로 변경 가능)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 예시로 5 에폭 동안 훈련\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = torch.tensor([label for sublist in labels for label in sublist]).to(device)  # flatten labels\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# 여기서는 훈련된 모델을 얻었어요. 이를 사용하여 객체 감지를 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# 데이터셋과 어노테이션 파일의 경로\n",
    "train_images_dir = '/home/dblab/coco/train2017'\n",
    "val_images_dir = '/home/dblab/coco/val2017'\n",
    "train_annotation_path = '/home/dblab/coco/annotations/instances_train2017.json'\n",
    "val_annotation_path = '/home/dblab/coco/annotations/instances_val2017.json'\n",
    "\n",
    "# COCO 데이터셋 로드\n",
    "train_coco = COCO(train_annotation_path)\n",
    "val_coco = COCO(val_annotation_path)\n",
    "\n",
    "# 클래스 목록 가져오기\n",
    "categories = train_coco.loadCats(train_coco.getCatIds())\n",
    "print(categories)\n",
    "categories_names = [category['name'] for category in categories]\n",
    "\n",
    "# 클래스를 숫자 라벨로 매핑\n",
    "category_dict = {category['id']: i + 1 for i, category in enumerate(categories)}\n",
    "\n",
    "# 데이터셋 변환\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# COCO 데이터셋 로더 설정\n",
    "train_dataset = CocoDetection(root=train_images_dir, annFile=train_annotation_path, transform=data_transform)\n",
    "val_dataset = CocoDetection(root=val_images_dir, annFile=val_annotation_path, transform=data_transform)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Faster R-CNN 모델 초기화\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 학습\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 검증\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    for images, targets in val_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += losses.item()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss/len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import new_datasets.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib.patches import Rectangle\n",
    "from new_datasets.coco_utils import ConvertCocoPolysToMask\n",
    "from util.label_info import coco_color_array, coco_label_list\n",
    "\n",
    "\n",
    "class COCODatasetV1(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms, visualization=False):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        # 117266\n",
    "        self.ids = list(sorted(self.coco.imgToAnns.keys()))\n",
    "        self._parses = ConvertCocoPolysToMask()\n",
    "        self._transforms = transforms\n",
    "        self._visualization = visualization\n",
    "        if self._visualization:\n",
    "            self.coco_color = coco_color_array\n",
    "            self.coco_label = coco_label_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super().__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        img, target = self._parses(img, target)\n",
    "        if self._transforms is not None:\n",
    "            img, target = self._transforms(img, target)\n",
    "        if self._visualization:\n",
    "            self.visualize(img, target)\n",
    "        return img, target\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batch = list(zip(*batch))\n",
    "        batch[0] = self.batched_tensor_from_tensor_list(batch[0])\n",
    "        return batch\n",
    "\n",
    "    def max_by_axis(self, the_list):\n",
    "        maxes = the_list[0]\n",
    "        for sublist in the_list[1:]:\n",
    "            for index, item in enumerate(sublist):\n",
    "                maxes[index] = max(maxes[index], item)\n",
    "        return maxes\n",
    "\n",
    "    def batched_tensor_from_tensor_list(self, images, size_divisible=32):\n",
    "        # if torchvision._is_tracing():\n",
    "        #     # batch_images() does not export well to ONNX\n",
    "        #     # call _onnx_batch_images() instead\n",
    "        #     return self._onnx_batch_images(images, size_divisible)\n",
    "\n",
    "        max_size = self.max_by_axis([list(img.shape) for img in images])\n",
    "        stride = float(size_divisible)\n",
    "        max_size = list(max_size)\n",
    "        max_size[1] = int(math.ceil(float(max_size[1]) / stride) * stride)\n",
    "        max_size[2] = int(math.ceil(float(max_size[2]) / stride) * stride)\n",
    "\n",
    "        batch_shape = [len(images)] + max_size\n",
    "        batched_imgs = images[0].new_full(batch_shape, 0)\n",
    "        for i in range(batched_imgs.shape[0]):\n",
    "            img = images[i]\n",
    "            batched_imgs[i, : img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "        return batched_imgs\n",
    "\n",
    "    def visualize(self, img, target):\n",
    "\n",
    "        # De-Normalize\n",
    "        if isinstance(self._transforms.transforms[-1], T.Normalize):\n",
    "            std = self._transforms.transforms[-1].std\n",
    "            mean = self._transforms.transforms[-1].mean\n",
    "\n",
    "            # numpy\n",
    "            img_np = np.array(img.permute(1, 2, 0), np.float32)  # C, W, H\n",
    "            img_np *= np.array(std)\n",
    "            img_np += np.array(mean)\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "        else:\n",
    "            img_np = np.array(img.permute(1, 2, 0), np.float32)\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "        # visualize img\n",
    "        plt.figure('input')\n",
    "        plt.imshow(img_np)\n",
    "\n",
    "        # visualize target\n",
    "        boxes = target['boxes']\n",
    "        labels = target['labels']\n",
    "        print('num objects : {}'.format(len(boxes)))\n",
    "\n",
    "        for i in range(len(boxes)):\n",
    "\n",
    "            x1 = boxes[i][0]\n",
    "            y1 = boxes[i][1]\n",
    "            x2 = boxes[i][2]\n",
    "            y2 = boxes[i][3]\n",
    "\n",
    "            # labels\n",
    "            plt.text(x=x1 - 5,\n",
    "                    y=y1 - 5,\n",
    "                    s=str(self.coco_label[labels[i]]),\n",
    "                    bbox=dict(boxstyle='round4',\n",
    "                            facecolor=self.coco_color[labels[i]],\n",
    "                            alpha=0.9))\n",
    "\n",
    "            # boxes\n",
    "            plt.gca().add_patch(Rectangle(xy=(x1, y1),\n",
    "                                        width=x2 - x1,\n",
    "                                        height=y2 - y1,\n",
    "                                        linewidth=1,\n",
    "                                        edgecolor=self.coco_color[labels[i]],\n",
    "                                        facecolor='none'))\n",
    "\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root = \"/home/dblab/coco\"\n",
    "    image_set = \"train\"\n",
    "\n",
    "    img_folder = os.path.join(root, f'{image_set}2017')\n",
    "    ann_file = os.path.join(root, 'annotations', f'instances_{image_set}2017.json')\n",
    "    transforms = T.Compose([\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomResize([800], max_size=1333),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    dataset = COCODatasetV1(img_folder, ann_file, transforms)\n",
    "\n",
    "    img, target = dataset.__getitem__(0)\n",
    "    print(\"the shape of imgs :\", img.size())\n",
    "    print(\"target['boxes'] :\", target['boxes'])\n",
    "    print(\"target keys :\", target.keys())\n",
    "    print(\"len: \", dataset.__len__())\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    train_batch_sampler = torch.utils.data.BatchSampler(train_sampler,\n",
    "                                                        batch_size=2,\n",
    "                                                        drop_last=True)\n",
    "    data_loader = DataLoader(dataset,\n",
    "                            batch_sampler=train_batch_sampler,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=dataset.collate_fn)\n",
    "\n",
    "    for i, (img, target) in enumerate(data_loader):\n",
    "        print(img.shape)\n",
    "        '''\n",
    "        torch.Size([2, 3, 800, 1152])\n",
    "        torch.Size([2, 3, 800, 1088])\n",
    "        torch.Size([2, 3, 1056, 1216])\n",
    "        torch.Size([2, 3, 800, 1088])\n",
    "        '''\n",
    "        if i == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
