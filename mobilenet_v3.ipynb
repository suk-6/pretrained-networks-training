{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "from pycocotools import mask as coco_mask\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "\"\"\"\n",
    "Transforms and data augmentation for both image + bbox.\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import pickle\n",
    "from packaging import version\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch import Tensor\n",
    "\n",
    "# needed due to empty tensor bug in pytorch and torchvision 0.5\n",
    "import torchvision\n",
    "if version.parse(torchvision.__version__) < version.parse('0.7'):\n",
    "    from torchvision.ops import _new_empty_tensor\n",
    "    from torchvision.ops.misc import _output_size\n",
    "\n",
    "def interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n",
    "    if version.parse(torchvision.__version__) < version.parse('0.7'):\n",
    "        if input.numel() > 0:\n",
    "            return torch.nn.functional.interpolate(\n",
    "                input, size, scale_factor, mode, align_corners\n",
    "            )\n",
    "\n",
    "        output_shape = _output_size(2, input, size, scale_factor)\n",
    "        output_shape = list(input.shape[:-2]) + list(output_shape)\n",
    "        return _new_empty_tensor(input, output_shape)\n",
    "    else:\n",
    "        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)\n",
    "\n",
    "def crop(image, target, region):\n",
    "    cropped_image = F.crop(image, *region)\n",
    "\n",
    "    target = target.copy()\n",
    "    i, j, h, w = region\n",
    "\n",
    "    # should we do something wrt the original size?\n",
    "    target[\"size\"] = torch.tensor([h, w])\n",
    "\n",
    "    fields = [\"labels\", \"area\", \"iscrowd\"]\n",
    "\n",
    "    if \"boxes\" in target:\n",
    "        boxes = target[\"boxes\"]\n",
    "        max_size = torch.as_tensor([w, h], dtype=torch.float32)\n",
    "        cropped_boxes = boxes - torch.as_tensor([j, i, j, i])\n",
    "        cropped_boxes = torch.min(cropped_boxes.reshape(-1, 2, 2), max_size)\n",
    "        cropped_boxes = cropped_boxes.clamp(min=0)\n",
    "        area = (cropped_boxes[:, 1, :] - cropped_boxes[:, 0, :]).prod(dim=1)\n",
    "        target[\"boxes\"] = cropped_boxes.reshape(-1, 4)\n",
    "        target[\"area\"] = area\n",
    "        fields.append(\"boxes\")\n",
    "\n",
    "    if \"masks\" in target:\n",
    "        # FIXME should we update the area here if there are no boxes?\n",
    "        target['masks'] = target['masks'][:, i:i + h, j:j + w]\n",
    "        fields.append(\"masks\")\n",
    "\n",
    "    # remove elements for which the boxes or masks that have zero area\n",
    "    if \"boxes\" in target or \"masks\" in target:\n",
    "        # favor boxes selection when defining which elements to keep\n",
    "        # this is compatible with previous implementation\n",
    "        if \"boxes\" in target:\n",
    "            cropped_boxes = target['boxes'].reshape(-1, 2, 2)\n",
    "            keep = torch.all(cropped_boxes[:, 1, :] > cropped_boxes[:, 0, :], dim=1)\n",
    "        else:\n",
    "            keep = target['masks'].flatten(1).any(1)\n",
    "\n",
    "        for field in fields:\n",
    "            target[field] = target[field][keep]\n",
    "\n",
    "    return cropped_image, target\n",
    "\n",
    "\n",
    "def hflip(image, target):\n",
    "    flipped_image = F.hflip(image)\n",
    "\n",
    "    w, h = image.size\n",
    "\n",
    "    target = target.copy()\n",
    "    if \"boxes\" in target:\n",
    "        boxes = target[\"boxes\"]\n",
    "        boxes = boxes[:, [2, 1, 0, 3]] * torch.as_tensor([-1, 1, -1, 1]) + torch.as_tensor([w, 0, w, 0])\n",
    "        target[\"boxes\"] = boxes\n",
    "\n",
    "    if \"masks\" in target:\n",
    "        target['masks'] = target['masks'].flip(-1)\n",
    "\n",
    "    return flipped_image, target\n",
    "\n",
    "\n",
    "def resize(image, target, size, max_size=None):\n",
    "    # size can be min_size (scalar) or (w, h) tuple\n",
    "\n",
    "    def get_size_with_aspect_ratio(image_size, size, max_size=None):\n",
    "        w, h = image_size\n",
    "        if max_size is not None:\n",
    "            min_original_size = float(min((w, h)))\n",
    "            max_original_size = float(max((w, h)))\n",
    "            if max_original_size / min_original_size * size > max_size:\n",
    "                size = int(round(max_size * min_original_size / max_original_size))\n",
    "\n",
    "        if (w <= h and w == size) or (h <= w and h == size):\n",
    "            return (h, w)\n",
    "\n",
    "        if w < h:\n",
    "            ow = size\n",
    "            oh = int(size * h / w)\n",
    "        else:\n",
    "            oh = size\n",
    "            ow = int(size * w / h)\n",
    "\n",
    "        return (oh, ow)\n",
    "\n",
    "    def get_size(image_size, size, max_size=None):\n",
    "        if isinstance(size, (list, tuple)):\n",
    "            return size[::-1]\n",
    "        else:\n",
    "            return get_size_with_aspect_ratio(image_size, size, max_size)\n",
    "\n",
    "    size = get_size(image.size, size, max_size)\n",
    "    rescaled_image = F.resize(image, size)\n",
    "\n",
    "    if target is None:\n",
    "        return rescaled_image, None\n",
    "\n",
    "    ratios = tuple(float(s) / float(s_orig) for s, s_orig in zip(rescaled_image.size, image.size))\n",
    "    ratio_width, ratio_height = ratios\n",
    "\n",
    "    target = target.copy()\n",
    "    if \"boxes\" in target:\n",
    "        boxes = target[\"boxes\"]\n",
    "        scaled_boxes = boxes * torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height])\n",
    "        target[\"boxes\"] = scaled_boxes\n",
    "\n",
    "    if \"area\" in target:\n",
    "        area = target[\"area\"]\n",
    "        scaled_area = area * (ratio_width * ratio_height)\n",
    "        target[\"area\"] = scaled_area\n",
    "\n",
    "    h, w = size\n",
    "    target[\"size\"] = torch.tensor([h, w])\n",
    "\n",
    "    if \"masks\" in target:\n",
    "        target['masks'] = interpolate(\n",
    "            target['masks'][:, None].float(), size, mode=\"nearest\")[:, 0] > 0.5\n",
    "\n",
    "    return rescaled_image, target\n",
    "\n",
    "\n",
    "def pad(image, target, padding):\n",
    "    # assumes that we only pad on the bottom right corners\n",
    "    padded_image = F.pad(image, (0, 0, padding[0], padding[1]))\n",
    "    if target is None:\n",
    "        return padded_image, None\n",
    "    target = target.copy()\n",
    "    # should we do something wrt the original size?\n",
    "    target[\"size\"] = torch.tensor(padded_image.size[::-1])\n",
    "    if \"masks\" in target:\n",
    "        target['masks'] = torch.nn.functional.pad(target['masks'], (0, padding[0], 0, padding[1]))\n",
    "    return padded_image, target\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size, max_size=None):\n",
    "        if isinstance(size, (list, tuple)):\n",
    "            self.size = size\n",
    "            self.max_size = None\n",
    "        else:\n",
    "            self.size = size\n",
    "            self.max_size = max_size\n",
    "\n",
    "    def __call__(self, img, target=None):\n",
    "        size = self.size\n",
    "        return resize(img, target, size, self.max_size)\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        region = T.RandomCrop.get_params(img, self.size)\n",
    "        return crop(img, target, region)\n",
    "\n",
    "\n",
    "class RandomSizeCrop(object):\n",
    "    def __init__(self, min_size: int, max_size: int):\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, img: PIL.Image.Image, target: dict):\n",
    "        w = random.randint(self.min_size, min(img.width, self.max_size))\n",
    "        h = random.randint(self.min_size, min(img.height, self.max_size))\n",
    "        region = T.RandomCrop.get_params(img, [h, w])\n",
    "        return crop(img, target, region)\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        image_width, image_height = img.size\n",
    "        crop_height, crop_width = self.size\n",
    "        crop_top = int(round((image_height - crop_height) / 2.))\n",
    "        crop_left = int(round((image_width - crop_width) / 2.))\n",
    "        return crop(img, target, (crop_top, crop_left, crop_height, crop_width))\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        if random.random() < self.p:\n",
    "            return hflip(img, target)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "class RandomResize(object):\n",
    "    def __init__(self, sizes, max_size=None):\n",
    "        assert isinstance(sizes, (list, tuple))\n",
    "        self.sizes = sizes\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, img, target=None):\n",
    "        size = random.choice(self.sizes)\n",
    "        return resize(img, target, size, self.max_size)\n",
    "\n",
    "\n",
    "class RandomPad(object):\n",
    "    def __init__(self, max_pad):\n",
    "        self.max_pad = max_pad\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        pad_x = random.randint(0, self.max_pad)\n",
    "        pad_y = random.randint(0, self.max_pad)\n",
    "        return pad(img, target, (pad_x, pad_y))\n",
    "\n",
    "\n",
    "class RandomSelect(object):\n",
    "    \"\"\"\n",
    "    Randomly selects between transforms1 and transforms2,\n",
    "    with probability p for transforms1 and (1 - p) for transforms2\n",
    "    \"\"\"\n",
    "    def __init__(self, transforms1, transforms2, p=0.5):\n",
    "        self.transforms1 = transforms1\n",
    "        self.transforms2 = transforms2\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        if random.random() < self.p:\n",
    "            return self.transforms1(img, target)\n",
    "        return self.transforms2(img, target)\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, img, target):\n",
    "        return F.to_tensor(img), target\n",
    "\n",
    "\n",
    "class RandomErasing(object):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.eraser = T.RandomErasing(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        return self.eraser(img), target\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, image, target=None):\n",
    "        image = F.normalize(image, mean=self.mean, std=self.std)\n",
    "        if target is None:\n",
    "            return image, None\n",
    "\n",
    "        # # FIXME : x1y1x2y2 zero2wh boxes to cxcywh zero21 boxes\n",
    "        # target = target.copy()\n",
    "        # h, w = image.shape[-2:]\n",
    "        # if \"boxes\" in target:\n",
    "        #     boxes = target[\"boxes\"]\n",
    "        #     boxes = box_xyxy_to_cxcywh(boxes)\n",
    "        #     boxes = boxes / torch.tensor([w, h, w, h], dtype=torch.float32)\n",
    "        #     target[\"boxes\"] = boxes\n",
    "\n",
    "        target = target.copy()\n",
    "        h, w = image.shape[-2:]\n",
    "        if \"boxes\" in target:\n",
    "            boxes = target[\"boxes\"]\n",
    "            boxes = boxes / torch.tensor([w, h, w, h], dtype=torch.float32)\n",
    "            target[\"boxes\"] = boxes\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + \"(\"\n",
    "        for t in self.transforms:\n",
    "            format_string += \"\\n\"\n",
    "            format_string += \"    {0}\".format(t)\n",
    "        format_string += \"\\n)\"\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coco_poly_to_mask(segmentations, height, width):\n",
    "    masks = []\n",
    "    for polygons in segmentations:\n",
    "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
    "        mask = coco_mask.decode(rles)\n",
    "        if len(mask.shape) < 3:\n",
    "            mask = mask[..., None]\n",
    "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "        mask = mask.any(dim=2)\n",
    "        masks.append(mask)\n",
    "    if masks:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "    else:\n",
    "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "    return masks\n",
    "\n",
    "class ConvertCocoPolysToMask:\n",
    "    def __call__(self, image, target):\n",
    "        w, h = image.size\n",
    "\n",
    "        image_id = target[\"image_id\"]\n",
    "        image_id = torch.tensor([image_id])\n",
    "\n",
    "        anno = target[\"annotations\"]\n",
    "\n",
    "        anno = [obj for obj in anno if obj[\"iscrowd\"] == 0]\n",
    "\n",
    "        boxes = [obj[\"bbox\"] for obj in anno]\n",
    "        # guard against no boxes via resizing\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
    "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
    "\n",
    "        classes = [obj[\"category_id\"] for obj in anno]\n",
    "        classes = torch.tensor(classes, dtype=torch.int64)\n",
    "\n",
    "        segmentations = [obj[\"segmentation\"] for obj in anno]\n",
    "        masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
    "\n",
    "        keypoints = None\n",
    "        if anno and \"keypoints\" in anno[0]:\n",
    "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
    "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "            num_keypoints = keypoints.shape[0]\n",
    "            if num_keypoints:\n",
    "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
    "\n",
    "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
    "        boxes = boxes[keep]\n",
    "        classes = classes[keep]\n",
    "        masks = masks[keep]\n",
    "        if keypoints is not None:\n",
    "            keypoints = keypoints[keep]\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = classes\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        if keypoints is not None:\n",
    "            target[\"keypoints\"] = keypoints\n",
    "\n",
    "        # for conversion to coco api\n",
    "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
    "        iscrowd = torch.tensor([obj[\"iscrowd\"] for obj in anno])\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODatasetV1(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        # 117266\n",
    "        self.ids = list(sorted(self.coco.imgToAnns.keys()))\n",
    "        self._parses = ConvertCocoPolysToMask()\n",
    "        self._transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super().__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        img, target = self._parses(img, target)\n",
    "        if self._transforms is not None:\n",
    "            img, target = self._transforms(img, target)\n",
    "        return img, target\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batch = list(zip(*batch))\n",
    "        batch[0] = self.batched_tensor_from_tensor_list(batch[0])\n",
    "        return batch\n",
    "\n",
    "    def max_by_axis(self, the_list):\n",
    "        maxes = the_list[0]\n",
    "        for sublist in the_list[1:]:\n",
    "            for index, item in enumerate(sublist):\n",
    "                maxes[index] = max(maxes[index], item)\n",
    "        return maxes\n",
    "\n",
    "    def batched_tensor_from_tensor_list(self, images, size_divisible=32):\n",
    "        # if torchvision._is_tracing():\n",
    "        #     # batch_images() does not export well to ONNX\n",
    "        #     # call _onnx_batch_images() instead\n",
    "        #     return self._onnx_batch_images(images, size_divisible)\n",
    "\n",
    "        max_size = self.max_by_axis([list(img.shape) for img in images])\n",
    "        stride = float(size_divisible)\n",
    "        max_size = list(max_size)\n",
    "        max_size[1] = int(math.ceil(float(max_size[1]) / stride) * stride)\n",
    "        max_size[2] = int(math.ceil(float(max_size[2]) / stride) * stride)\n",
    "\n",
    "        batch_shape = [len(images)] + max_size\n",
    "        batched_imgs = images[0].new_full(batch_shape, 0)\n",
    "        for i in range(batched_imgs.shape[0]):\n",
    "            img = images[i]\n",
    "            batched_imgs[i, : img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n",
    "        return batched_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "        Resize((640, 480)),\n",
    "        ToTensor(),\n",
    "        Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "rootDataset = os.path.join('/', 'home', 'dblab', 'coco')\n",
    "\n",
    "trainImagePath = os.path.join(rootDataset, 'train2017')\n",
    "trainAnnoPath = os.path.join(rootDataset, 'annotations', 'instances_train2017.json')\n",
    "valImagePath = os.path.join(rootDataset, 'val2017')\n",
    "valAnnoPath = os.path.join(rootDataset, 'annotations', 'instances_val2017.json')\n",
    "\n",
    "trainDataset = COCODatasetV1(trainImagePath, trainAnnoPath, transforms)\n",
    "valDataset = COCODatasetV1(valImagePath, valAnnoPath, transforms)\n",
    "\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=4, shuffle=True, collate_fn=trainDataset.collate_fn)\n",
    "valDataLoader = DataLoader(valDataset, batch_size=4, shuffle=True, collate_fn=valDataset.collate_fn)\n",
    "\n",
    "def showImage(image, target):\n",
    "    image = image.permute(1, 2, 0)\n",
    "    image = image.numpy()\n",
    "    image = image * np.array([0.229, 0.224, 0.225])\n",
    "    image = image + np.array([0.485, 0.456, 0.406])\n",
    "    image = image * 255\n",
    "    image = image.astype(np.uint8)\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    for box in target['boxes']:\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "# for image, target in trainDataLoader:\n",
    "#     showImage(image[0], target[0])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = models.resnet50(pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "lr = 0.0001\n",
    "epochs = 5\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "lossFunction = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "savePath = os.path.join('/', 'home', 'dblab', 'export')\n",
    "\n",
    "if os.path.exists(savePath) == False:\n",
    "    os.mkdir(savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(params):\n",
    "    model = fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "\n",
    "    # Unpack parameters\n",
    "    lossFunction = params[\"lossFunction\"]\n",
    "    trainDataLoader = params[\"trainDataLoader\"]\n",
    "    device = params[\"device\"]\n",
    "    optimizer = params[\"optimizer\"]\n",
    "    epochs = params[\"epochs\"]\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(tqdm(trainDataLoader)):\n",
    "            images, targets = data\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "\n",
    "            # Total loss calculation\n",
    "            total_loss = sum(loss for loss in loss_dict.values())\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {total_loss.item()}')\n",
    "\n",
    "    torch.save(model.state_dict(), f'{savePath}/fasterrcnn_mobilenet_v3_large_fpn_{timestamp}.pth')\n",
    "    print('Training completed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'epochs': epochs,\n",
    "    'optimizer': optimizer,\n",
    "    'lossFunction': lossFunction,\n",
    "    'trainDataLoader': trainDataLoader,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "train(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
